{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af348eba-b1ca-4b2b-a49a-ecd0cd2de78a",
   "metadata": {},
   "source": [
    "# Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3de6f-a30a-4a2e-91f4-593f1f41d88a",
   "metadata": {},
   "source": [
    "---\n",
    "In class today we will be implementing a Markov chain to process sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6dda45-e3c8-43b4-8362-e43292a3c6c3",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "\n",
    "1. Students will be able to explain the Markov Chain process\n",
    "1. Implement a Markov Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4394c-9c65-4ed4-bfe8-0db56ec32223",
   "metadata": {},
   "source": [
    "Markov Chains represent a series of events following the Markov Property: future states are memory-less in that they depend only on the current state. This can be expanded to the idea of variable order Markov models where there is a variable-length memory (eg. 1st order Markov Model). Markov models consist of fully observable states. \n",
    "\n",
    "> A common example of this is in predicting the weather: We can clearly see the current weather and would like to predict tomorrow's weather. This is also applicable to biology with one case being CpG islands. \n",
    "\n",
    "Our goal today will be to implement a Markov model built from words. For our example text, we will use the classic example of Dr. Seuss because of the repetitive nature of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69269a05-73d5-47e4-96f3-29e25ed3b928",
   "metadata": {},
   "source": [
    "---\n",
    "## Train Markov model\n",
    "\n",
    "For our initial implementation of the Markov Model, we will use the simple example of Dr. Seuss: \"One fish two fish red fish blue fish.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2e7a1-b676-4874-ad72-fe14edf98a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entry(markov_model: dict, word_one: str, word_two: str):\n",
    "    \"\"\"\n",
    "    Function to access the markov model and add 1 to word 1's inner dictionary entry for word 2\n",
    "    This adds 1 to the count of times where word 1 comes before word 2 in the text\n",
    "    \n",
    "    @param markov_model: dictionary of dictionaries (may be empty) mapping strings to a dict of {string: integer}\n",
    "    @param word_one: str, it's the word being used as the key to the outer dictionary\n",
    "    @param word_two: str, the word being used as the key to the inner dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # access the outer dictionary's value for word 1 (set it to a blank dictionary if the key doesn't exist)\n",
    "    markov_model[word_one] = markov_model.get(word_one, {})\n",
    "\n",
    "    # access the inner dictionary and attempt to access the key for word two (and set the count to 0)\n",
    "    # regardless of if the value existed before, we're adding 1 to it\n",
    "    markov_model[word_one][word_two] = markov_model[word_one].get(word_two, 0) + 1 # I SWEAR this isn't as ratchet as it looks!!!!\n",
    "\n",
    "    # now return the updated model\n",
    "    return markov_model\n",
    "\n",
    "\n",
    "def build_markov_model(markov_model: dict, new_text: str):\n",
    "    '''\n",
    "    Function to build or add to a 1st order Markov model given a string of text\n",
    "    We will store the markov model as a dictionary of dictionaries\n",
    "    The key in the outer dictionary represents the current state\n",
    "    and the inner dictionary represents the next state with their contents containing\n",
    "    the transition probabilities.\n",
    "    Note: This would be easier to read if we were to build a class representation\n",
    "           of the model rather than a dictionary of dictionaries, but for simplicitiy\n",
    "           our implementation will just use this structure.\n",
    "    \n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "        new_text (str): a string to build or add to the moarkov_model\n",
    "\n",
    "    Returns:\n",
    "        markov_model (dict of dicts): an updated markov_model\n",
    "        \n",
    "    Pseudocode:\n",
    "        Add artificial states for start and end\n",
    "        For each word in text:\n",
    "            Increment markov_model[word][next_word]\n",
    "        \n",
    "    '''\n",
    "\n",
    "    # split the line into separate words\n",
    "    sentence = new_text.split()\n",
    "\n",
    "    # adds the entry for the start state transitioning to the first word in the sentence\n",
    "    add_entry(markov_model, word_one = \"*S*\", word_two = sentence[0])\n",
    "\n",
    "    # iterate over the indices of the words in the sentence\n",
    "    for i in range(len(sentence) - 1):\n",
    "        # add the entry for word i transitioning to word i+1\n",
    "        add_entry(markov_model, word_one = sentence[i], word_two = sentence[i+1])\n",
    "    \n",
    "    # now that we're done with the words in the sentence, we just need to add the end state\n",
    "    add_entry(markov_model, word_one = sentence[-1], word_two = \"*E*\")\n",
    "\n",
    "    # return the trained model\n",
    "    return markov_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc64609-ae18-4d8e-acd9-0e6a6110bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'*S*': {'one': 1}, 'one': {'fish': 1}, 'fish': {'two': 1, 'red': 1, 'blue': 1, '*E*': 1}, 'two': {'fish': 1}, 'red': {'fish': 1}, 'blue': {'fish': 1}}\n"
     ]
    }
   ],
   "source": [
    "markov_model = dict()\n",
    "text = \"one fish two fish red fish blue fish\"\n",
    "markov_model = build_markov_model(markov_model, text)\n",
    "print (markov_model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7d42532-a75f-4ee4-aad0-fb9d6843050c",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "{'*S*': {'one': 1}, 'one': {'fish': 1}, 'fish': {'two': 1, 'red': 1, 'blue': 1, '*E*': 1}, 'two': {'fish': 1}, 'red': {'fish': 1}, 'blue': {'fish': 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36386041-90a1-4c7a-967c-ed08c38f56c0",
   "metadata": {},
   "source": [
    "###  Nth order Markov chain\n",
    "In the above model, each event or word is output from only the previous state with no memory of any prior states. While this is useful in some cases, typical biological applications of Markov chains require higher-order models to accurately capture what we know about a system. For instance, in attempting to identify coding regions of a genome, we know that open reading frames (ORFs) contain codon triplets, and so a third or sixth order Markov chain would better describe these regions. Here you will implement a generalized form of our previous Markov Chain to allow for Nth order chains.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "796cf4ce-29e9-4507-a7e0-afc0e9b1429c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T17:52:04.016115Z",
     "start_time": "2026-02-02T17:52:04.003656Z"
    }
   },
   "source": [
    "def add_multi(markov_model: dict, key_tuple: tuple[str], next_word: str):\n",
    "    \"\"\"\n",
    "    Function to access the markov model and add 1 to word 1's inner dictionary entry for word 2\n",
    "    This adds 1 to the count of times where word 1 comes before word 2 in the text\n",
    "    \n",
    "    @param markov_model: dictionary of dictionaries (may be empty) mapping strings to a dict of {string: integer}\n",
    "    @param key_tuple: tuple of strings (variable length), it's the sequences of words being used as the key to the outer dictionary\n",
    "    @param next_word: str, the word being used as the key to the inner dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # access the outer dictionary's value for word 1 (set it to a blank dictionary if the key doesn't exist)\n",
    "    markov_model[key_tuple] = markov_model.get(key_tuple, {})\n",
    "\n",
    "    # access the inner dictionary and attempt to access the key for word two (and set the count to 0)\n",
    "    # regardless of if the value existed before, we're adding 1 to it\n",
    "    markov_model[key_tuple][next_word] = markov_model[key_tuple].get(next_word, 0) + 1 # I SWEAR this isn't as ratchet as it looks!!!!\n",
    "\n",
    "    # now return the updated model\n",
    "    return markov_model\n",
    "\n",
    "\n",
    "def build_markov_model(markov_model, text, order=1):\n",
    "    '''\n",
    "    Function to build or add to a Nth order Markov model given a string of text\n",
    "    Still only takes one line of text as input\n",
    "    dict mapping tuples of N strings to dictionaries that map strings to ints\n",
    "    {tuple(str): {str: int}}\n",
    "    where N is the order\n",
    "\n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "            or None if a new model is being built\n",
    "        new_text (str): a string to build or add to the moarkov_model\n",
    "        order (int): the number of previous states to consider for the model\n",
    "        \n",
    "    Returns:\n",
    "        markov_model (dict of dicts): an updated/new markov_model\n",
    "    '''\n",
    "\n",
    "    # split the line of text into a list of words\n",
    "    sentence = text.split(\" \")\n",
    "\n",
    "    # check if our order is greater than the length of the line\n",
    "    if order > len(sentence):\n",
    "        # print the problem to stdout and return the still-untrained model\n",
    "        print(f\"The order {order} is too high for the line length of {len(sentence)} words! Quitting without training the model...\")\n",
    "        return markov_model\n",
    "\n",
    "    # since we know we're at the start of the line, map the start state to the first word and then slowly flush out the start states\n",
    "    for i in range(order):\n",
    "        # we need a decreasing number of start states in our tuple of the outter dictionary\n",
    "\n",
    "        # make a little list contianing all the instances of the start state that we need for whatever iteration we're currently on\n",
    "        start_list = [\"*S*\"] * (order - i)\n",
    "        # print(f\"Start list: {start_list}\")\n",
    "\n",
    "        # iterate over the words in the sentence that we have already hit\n",
    "        for word in sentence[:i]:\n",
    "            # add the words we've already hit to the key\n",
    "            start_list.append(word)\n",
    "        # print(f\"after adding words: {start_list}\")\n",
    "\n",
    "        # now convert the key, currently a list, to a tuple\n",
    "        key = tuple(start_list)\n",
    "\n",
    "        # add this key to the markov model\n",
    "        markov_model[key] = markov_model.get(key, {})\n",
    "\n",
    "        # add entry to the markov model mapping this key to the next word in the sentence\n",
    "        add_multi(markov_model, key, sentence[i])\n",
    "\n",
    "    # now that we're out of that for loop, we can make our keys entirely out of words that are in the list\n",
    "    # iterate over the indices of the words in the list\n",
    "    for j in range(len(sentence) - order):\n",
    "        # prepare the tuple that we'll use as a key, which includes N words from the line\n",
    "        key = tuple(sentence[j:j+order])\n",
    "        # print(f\"key: {key} // value: {sentence[j+order]}\")\n",
    "\n",
    "        # add an entry mapping the current N words in the window to the next word after\n",
    "        add_multi(markov_model, key, sentence[j+order])\n",
    "\n",
    "    # now we are missing the last entry in our dict that maps the last N words to the end state\n",
    "    key = tuple(sentence[-1 * order:])\n",
    "    add_multi(markov_model, key, \"*E*\")\n",
    "\n",
    "    # return the updated markov model\n",
    "    return markov_model\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "62b8f110-6cad-41bb-b976-261a136282ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T17:52:10.397284Z",
     "start_time": "2026-02-02T17:52:10.364779Z"
    }
   },
   "source": [
    "markov_model = dict()\n",
    "text = \"one fish two fish red fish blue red fish blue\"\n",
    "markov_model = build_markov_model(markov_model, text, order=2)\n",
    "markov_model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('*S*', '*S*'): {'one': 1},\n",
       " ('*S*', 'one'): {'fish': 1},\n",
       " ('one', 'fish'): {'two': 1},\n",
       " ('fish', 'two'): {'fish': 1},\n",
       " ('two', 'fish'): {'red': 1},\n",
       " ('fish', 'red'): {'fish': 1},\n",
       " ('red', 'fish'): {'blue': 2},\n",
       " ('fish', 'blue'): {'red': 1, '*E*': 1},\n",
       " ('blue', 'red'): {'fish': 1}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "raw",
   "id": "0bf821ed-d84f-4fe7-9de9-3c21a3841aa3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Expected output:\n",
    "{('*S*', '*S*'): {'one': 1},\n",
    " ('*S*', 'one'): {'fish': 1},\n",
    " ('one', 'fish'): {'two': 1},\n",
    " ('fish', 'two'): {'fish': 1},\n",
    " ('two', 'fish'): {'red': 1},\n",
    " ('fish', 'red'): {'fish': 1},\n",
    " ('red', 'fish'): {'blue': 1},\n",
    " ('fish', 'blue'): {'fish': 1},\n",
    " ('blue', 'fish'): {'*E*': 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14699827-2b59-4490-a961-a9ed04351a8e",
   "metadata": {},
   "source": [
    "## Generate text from Markov Model\n",
    "\n",
    "Markov models are \"generative models\". That is, the probability states in the model can be used to generate output following the conditional probabilities in the model.\n",
    "\n",
    "We will now generate a sequence of text from the Markov model. For this section, I recommend using np.random.choice, which allows for you to provide a probability distribution for drawing the next edge in the chain."
   ]
  },
  {
   "cell_type": "code",
   "id": "baea1038-ff96-46d4-b61a-cbd0777263f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T18:34:39.006475Z",
     "start_time": "2026-02-02T18:34:38.992355Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_probabilities(word_counts: dict[str, int]):\n",
    "    \"\"\"\n",
    "    Function to calculate the frequency of words, given a dictionary indicating how many times each word occurs\n",
    "\n",
    "    Frequencies get calculated by summing all the counts in the dict and dividing each count by that sum\n",
    "\n",
    "    @param word_counts: dictionary mapping strings to integers, the ints are the counts of how many times each of those strings occurs\n",
    "    @return: dictionary mapping strings to floats, where each float is the frequency (decimal form) of the word\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate the sum of all the words' counts\n",
    "    total = sum(word_counts.values())  # <- change is that we are overwritiing built in function sum so i changed `sum` to `total`\n",
    "\n",
    "    # initialize the frequency table\n",
    "    frequencies = {}\n",
    "\n",
    "    # now iterate over the key-value pairs in the input dictionary\n",
    "    for word, count in word_counts.items():  # <- need to add `.items()` as we are attempting to get key and value\n",
    "        # get the frequency of this word and store it in the dict\n",
    "        frequencies[word] = count/total # <- again `sum` needs to be `total`\n",
    "\n",
    "    return frequencies\n",
    "\n",
    "\n",
    "def get_next_word(current_word, markov_model, seed=42):\n",
    "    '''\n",
    "    Function to randomly move a valid next state given a markov model\n",
    "    and a current state (word)\n",
    "    \n",
    "    Args: \n",
    "        current_word (tuple): a word that exists in our model\n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "\n",
    "    Returns:\n",
    "        next_word (str): a randomly selected next word based on transition probabilies\n",
    "        \n",
    "    Pseudocode:\n",
    "        Calculate transition probilities for all next states from a given state (counts/sum)\n",
    "            Initialize a probability dictionary\n",
    "            Access the value in the outer dict associated with our current word\n",
    "            Once there, we just sum all of the values in current word's inner dictionary\n",
    "            And then for each word that's a key in the inner dict, we take its count and divide by the sum\n",
    "            Now in the probability dict, map that key from the inner dict to the number we just didvided for it <3\n",
    "        Randomly draw from these to generate the next state\n",
    "            Just pass the values from the probability dict to np.random.choice\n",
    "        \n",
    "    '''\n",
    "    # access the part of the markov model with all the edges that come from the current word\n",
    "    if current_word not in markov_model:\n",
    "        return None\n",
    "\n",
    "    edge_counts = markov_model[current_word]\n",
    "\n",
    "\n",
    "    # calculate the probabilities for each of the edges\n",
    "    probability_dict = calculate_probabilities(edge_counts)\n",
    "\n",
    "    # randomly select words from the probability dict\n",
    "    next_word = str(np.random.choice(a=list(probability_dict.keys()), p=list(probability_dict.values())))\n",
    "\n",
    "    # return the word\n",
    "    return next_word\n",
    "\n",
    "def generate_random_text(markov_model, seed=42):\n",
    "    '''\n",
    "    Function to generate text given a markov model\n",
    "    \n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "\n",
    "    Returns:\n",
    "        sentence (str): a randomly generated sequence given the model\n",
    "        \n",
    "    Pseudocode:\n",
    "        Initialize sentence at start state\n",
    "        Until End State:\n",
    "            append get_next_word(current_word, markov_model)\n",
    "        Return sentence\n",
    "        \n",
    "    '''\n",
    "    # initialize our output string\n",
    "    generated_text = []\n",
    "\n",
    "    # start at the start state\n",
    "    curr_word = \"*S*\"\n",
    "\n",
    "    # start a loop that runs until we hit the end state\n",
    "    while curr_word != \"*E*\":\n",
    "        # append the current word\n",
    "        generated_text.append(curr_word)\n",
    "\n",
    "        # switch to the next word\n",
    "        curr_word = get_next_word(curr_word, markov_model, seed)\n",
    "\n",
    "    # now that we've broken from the loop, we need to append the end state\n",
    "    generated_text.append(curr_word)\n",
    "    \n",
    "    # now join all the text and ship it\n",
    "    return \" \".join(generated_text)"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "4b8724eb-5e10-4ffc-a76e-8a2015b3ee47",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9280c1d-9f9e-47d8-8697-cc6255b3ec7d",
   "metadata": {},
   "source": [
    "## All the Fish\n",
    "Up till now, you have only been working with a line or two of the Dr. Seuss' _One Fish, Two Fish_. Now, I want you to build a model using the whole book and try different orders of Markov models."
   ]
  },
  {
   "cell_type": "code",
   "id": "c04a137e-4c1d-40df-9056-177c022009ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T19:21:46.341885Z",
     "start_time": "2026-02-02T19:21:46.305630Z"
    }
   },
   "source": [
    "\n",
    "def update_markov_model(markov_model, text, order: int = 1, prev_line: str = None):\n",
    "    '''\n",
    "    Function to build or add to a Nth order Markov model given a string of text\n",
    "    dict mapping tuples of N strings to dictionaries that map strings to ints\n",
    "    {tuple(str): {str: int}}\n",
    "    where N is the order\n",
    "\n",
    "    This assumes that there will be more lines after the one that this reads\n",
    "    This is resilient against not knowing whether the current line is the first in the file or if it's not\n",
    "\n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "            or None if a new model is being built\n",
    "        text (str): a string to build or add to the moarkov_model\n",
    "        order (int): the number of previous states to consider for the model\n",
    "        prev_line (str): the previous line of text (optional)\n",
    "        \n",
    "    Returns:\n",
    "        markov_model (dict of dicts): an updated/new markov_model\n",
    "        tuple of the last N words in the line so the next line isn't treated as the start of the file\n",
    "    '''\n",
    "\n",
    "    # split the line of text into a list of words\n",
    "    sentence = text.split(\" \")\n",
    "\n",
    "    # check if our order is greater than the length of the line\n",
    "    if order > len(sentence):\n",
    "        # print the problem to stdout and return the still-untrained model\n",
    "        print(sentence)\n",
    "        print(f\"The order {order} is too high for the line length of {len(sentence)} words! Quitting without training the model...\")\n",
    "        return markov_model\n",
    "\n",
    "    # since we know we're at the start of the line, map the start state to the first word and then slowly flush out the start states\n",
    "    for i in range(order):\n",
    "        # we need a decreasing number of start states in our tuple of the outter dictionary\n",
    "\n",
    "        # check if we have a previous line to pull from\n",
    "        if prev_line:\n",
    "            # print(f\"HEY PREV_LINE WAS TRUE BTW AT I = {i}\")\n",
    "\n",
    "            # take a slice of the last order - i words and make that a list\n",
    "            start_list = prev_line.split(\" \")[-1 * (order - i):]\n",
    "\n",
    "        else:\n",
    "            # since we don't have a previous line to pull from, just use the start state (we're at the start of the file)\n",
    "            # make a little list contianing all the instances of the start state that we need for whatever iteration we're currently on\n",
    "            start_list = [\"*S*\"] * (order - i)\n",
    "            # print(f\"Start list: {start_list}\")\n",
    "\n",
    "        # iterate over the words in the sentence that we have already hit\n",
    "        for word in sentence[:i]:\n",
    "            # add the words we've already hit to the key\n",
    "            start_list.append(word)\n",
    "        # print(f\"after adding words: {start_list}\")\n",
    "\n",
    "        # now convert the key, currently a list, to a tuple\n",
    "        key = tuple(start_list)\n",
    "        # print(f\"Key: {key}\")\n",
    "        \n",
    "\n",
    "        # add this key to the markov model\n",
    "        markov_model[key] = markov_model.get(key, {})\n",
    "\n",
    "        # add entry to the markov model mapping this key to the next word in the sentence\n",
    "        add_multi(markov_model, key, sentence[i])\n",
    "\n",
    "    # now that we're out of that for loop, we can make our keys entirely out of words that are in the list\n",
    "    # iterate over the indices of the words in the list\n",
    "    for j in range(len(sentence) - order):\n",
    "        # prepare the tuple that we'll use as a key, which includes N words from the line\n",
    "        key = tuple(sentence[j:j+order])\n",
    "        # print(f\"key: {key} // value: {sentence[j+order]}\")\n",
    "\n",
    "        # add an entry mapping the current N words in the window to the next word after\n",
    "        add_multi(markov_model, key, sentence[j+order])\n",
    "\n",
    "    # now we are missing the last entry in our dict that maps the last N words to the end state\n",
    "    # key = tuple(sentence[-1 * order:])\n",
    "    # add_multi(markov_model, key, \"*E*\")\n",
    "\n",
    "    # return the updated markov model\n",
    "    return markov_model\n",
    "\n",
    "\n",
    "def train_model(model: dict, path: str, order: int = 1):\n",
    "    \"\"\"\n",
    "    Function to train a markov model off of the supplied text file\n",
    "    Takes a dict as input so you can update a model based on another file without overwriting it\n",
    "\n",
    "    @param model: dict if empty, or dict mapping tuples to dictionaries of string:int\n",
    "    @param path: str, the relative path to the text file the model is being trained on\n",
    "    @param order: int, the order of the Markov model (i.e. how many words long is the key in the outer dict)\n",
    "    @return: a trained markov model, a dict of dicts, where the key to the outer dict is a tuple of strings \n",
    "             and the inner dict maps strings to ints\n",
    "    \"\"\"\n",
    "    # Initialize the previous line\n",
    "    prev_line = None\n",
    "\n",
    "    # open the file\n",
    "    with open(path, mode=\"r\", encoding=\"utf-8\") as book:\n",
    "        # iterate over the lines of the file\n",
    "        for line in book:\n",
    "            # add the data from the \n",
    "            trained_model = update_markov_model(model, line, order, prev_line)\n",
    "            \n",
    "            # update the previous line before moving to the next\n",
    "            prev_line=line\n",
    "\n",
    "    # send the model to the end state\n",
    "    trained_model = update_markov_model(model, \"*E*\", order, prev_line=prev_line)\n",
    "\n",
    "    # pass the trained model\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "def generate_text(markov_model, order: int = 1, seed: int = 42):\n",
    "    '''\n",
    "    Function to generate text given a markov model\n",
    "    Why does this exist and how is it different from generate_random_text()?\n",
    "        *S* and *E* need to be tuples for our parameterized\n",
    "    \n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "        order: (int) an int indicating what order of markov model we're working with\n",
    "\n",
    "    Returns:\n",
    "        sentence (str): a randomly generated sequence given the model\n",
    "        \n",
    "    Pseudocode:\n",
    "        Initialize sentence at start state\n",
    "        Until End State:\n",
    "            append get_next_word(current_word, markov_model)\n",
    "        Return sentence\n",
    "        \n",
    "    '''\n",
    "    # initialize our output string\n",
    "    generated_text = []\n",
    "\n",
    "    # initialize start state\n",
    "    curr_state = [\"*S*\"] * order\n",
    "    curr_word = None\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # access whatever the next word is and add it to our generated text\n",
    "        #print(f\"Current state: {curr_state}\")\n",
    "        curr_key = tuple(curr_state)\n",
    "        #print(f\"Current key: {curr_key} // type: {type(curr_key)}\")\n",
    "        curr_word = get_next_word(curr_key, markov_model, seed)\n",
    "        if curr_word is None or curr_word == \"*E*\":\n",
    "            break\n",
    "        generated_text.append(curr_word)\n",
    "\n",
    "        # remove the 0th item in the curr_state list and add the next word to the end of the current state\n",
    "        del curr_state[0]\n",
    "        curr_state.append(curr_word)\n",
    "    \n",
    "    # now join all the text and ship it\n",
    "    return \" \".join(generated_text)\n",
    "\n",
    "\n",
    "\n",
    "# Now just add some more training data to the markov model. You can find it under data/one_fish_two_fish.txt\n",
    "\n",
    "markov_model = dict()\n",
    "# Read in the whole book\n",
    "text_file = \"data/one_fish_two_fish.txt\"\n",
    "first_order = train_model(markov_model, path=text_file, order=3)\n",
    "#print(first_order)\n",
    "#for key,val in first_order.items():\n",
    "#    print(f\"Key: {key} // Type: {type(key)} // Value: {val} // Type: {type(val)}\")\n",
    "#print(f\"Markov model: {type(first_order)}\")\n",
    "print (generate_text(first_order, order=3, seed=7))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SO...\\n']\n",
      "The order 3 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['SO...\\n']\n",
      "The order 3 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['So...\\n']\n",
      "The order 3 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['*E*']\n",
      "The order 3 is too high for the line length of 1 words! Quitting without training the model...\n",
      "One fish, Two fish, Red fish, Blue fish,\n",
      " Black fish, Blue fish, Old fish, New fish.\n",
      " This one has a yellow hat.\n",
      " From there to here,\n",
      " From here to there,\n",
      " Funny things are everywhere.\n",
      " These yellow pets are called the Zeds.\n",
      " They have one hair upon their heads.\n",
      " Their hair grows fast. So fast they say,\n",
      " They need a haircut every day.\n",
      " Who am I? My name is Ish\n",
      " On my hand I have a bird I like to hold.\n",
      " My shoe is off, my foot is cold.\n",
      " My shoe is off, my foot is cold.\n",
      " I have a bird I like to do is hop,\n",
      " From finger top to finger top.\n",
      " I hop from left to right and then...\n",
      " Hop, hop! I hop right back again.\n",
      " I like to hop, hop, hop?\n",
      " I do not know. Go ask your Pop.\n",
      " Brush, brush, brush, brush\n",
      " Comb, comb, comb, comb\n",
      " Blue hair is fun to brush and comb,\n",
      " Should have a pet like this at home.\n",
      " Who is this pet? Say! He is wet.\n",
      " You never yet met a pet, I bet,\n",
      " As wet as they let this wet pet get.\n",
      " Did you ever fly a kite in bed?\n",
      " did you ever walk with ten cats on your head?\n",
      " Did you ever milk this kind of cow?\n",
      " Well, we can do it. We know how.\n",
      " If you never did, you should.\n",
      " These things are fun, and fun is good.\n",
      " Hello, hello. Are you there?\n",
      " Hello! I called you up to say hello.\n",
      " I said Hello.\n",
      " Can you hear me, Joe?\n",
      " Oh no, I cannot hear your call.\n",
      " I cannot hear your call at all.\n",
      " This is not good, and I know why.\n",
      " A mouse has cut the wire, goodbye!\n",
      " From near to far, from here to there,\n",
      " Funny things are everywhere.\n",
      " These yellow pets are called the Zeds.\n",
      " They have one hair upon their heads.\n",
      " Their hair grows fast. So fast they say,\n",
      " They need a haircut every day.\n",
      " Who am I? My name is Ned\n",
      " I do not know, go ask your dad.\n",
      " Some are thin, and some are fat.\n",
      " The fat one has a yellow hat.\n",
      " From there to here,\n",
      " From here to there,\n",
      " Funny things are everywhere.\n",
      " These yellow pets are called the Zeds.\n",
      " They have one hair upon their heads.\n",
      " Their hair grows fast. So fast they say,\n",
      " They need a haircut every day.\n",
      " Who am I? My name is Ish\n",
      " On my hand I have a bird I like to box.\n",
      " So every day I box a Gox.\n",
      " In yellow socks I box my Gox.\n",
      " I box in yellow Gox box socks.\n",
      " It is fun to brush and comb.\n",
      " All girls who like to run.\n",
      " They run for fun in the hot, hot sun.\n",
      " Oh me! Oh my! Oh me! oh my!\n",
      " What a lot of ink,\n",
      " you should get a Yink, I think.\n",
      " Hop, hop, hop! I am a Yop\n",
      " All I like to box. How I like to have him in the house.\n",
      " At our house we play out back.\n",
      " We play a game called ring the Gack.\n",
      " Would you like to go Bump! Bump!\n",
      " Just jump on the hump of the Wump of Gump\n",
      " Who am I? My name is Ned\n",
      " I do not know. Go ask your Pop.\n",
      " Brush, brush, brush, brush\n",
      " Comb, comb, comb, comb\n",
      " Blue hair is fun to sing if you sing with a Ying.\n",
      " My Ying can sing like anything.\n",
      " I sing high and my Ying sings low.\n",
      " And we are not too bad, you know.\n",
      " this one, I think, is called a Yink.\n",
      " he likes to drink is ink.\n",
      " The ink he likes to drink is ink.\n",
      " The ink he likes to drink.\n",
      " He likes to drink, and drink, and drink.\n",
      " the thing he likes to drink is pink.\n",
      " He likes to wink and drink pink ink.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "9751e164-73d4-4c53-94d4-92f0d6f96b76",
   "metadata": {},
   "source": [
    "---\n",
    "## Pick Your Poison\n",
    "There are three texts provided for under `data/`. The first is:\n",
    "1. Dr. Seuss' \"One Fist, Two Fish\" (179 lines of text)\n",
    "2. All of Shakespeare's sonnets (2308 lines of text)\n",
    "3. Homer's \"The Odyssey\" (9255 Lines of text)"
   ]
  },
  {
   "cell_type": "code",
   "id": "1279dd44-0513-4cea-a51f-ac3b52927cad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T19:24:24.255027Z",
     "start_time": "2026-02-02T19:24:24.134576Z"
    }
   },
   "source": [
    "# An example of a more complex text that we can use to generate more complex output\n",
    "nth_order_markov_model = dict()\n",
    "#with open(\"data/odyssey.txt\", \"r\") as poison_text:\n",
    "\n",
    "    # Process the lines. Consider that sonnets are separated by an empty line.\n",
    "poison_text = \"data/odyssey.txt\"\n",
    "nth_order_markov_model = train_model(markov_model, path = poison_text, order=2)\n",
    " \n",
    "print (generate_text(nth_order_markov_model, order = 2, seed=7))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['again.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['father.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['me.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['henceforward.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['husband.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['again.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['it.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "[\"journey.'\\n\"]\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "[\"funeral.'\\n\"]\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['him.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['world.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['father.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['you.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['town.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['me.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['home.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['banquet.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['gods.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['seats.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['me.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "[\"land?'\\n\"]\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['Cyclops.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "[\"you.'\\n\"]\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "[\"goddess.'\\n\"]\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "[\"truly.'\\n\"]\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "[\"chance.'\\n\"]\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "[\"me.'\\n\"]\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['faster.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "[\"men.'\\n\"]\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "[\"world.'\\n\"]\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['me.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['sleep.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['obey.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['again.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['manhood.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['all.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['themselves.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['me.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['lies.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['plan:\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['Troy.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['angry.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['me.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['us.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['her.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['seats.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['father.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['it.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['revenge.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['said:\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['suitors.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['boyhood.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['sleep.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['iron.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['house.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['here.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['dreams.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['me.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['said:\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['wine.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['saying:\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['said:\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['here.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['eyelids.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['seat.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['spoke.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['others.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['again.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['misfortune.\"\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['sorrows.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['plant.\\n']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['*E*']\n",
      "The order 2 is too high for the line length of 1 words! Quitting without training the model...\n",
      "Tell me, too, about all this, my men made feast, and I will box, wrestle, or run, I do\n",
      " not go out against Ulysses,\n",
      " or you may be by nature. The man is not a goddess, and as he left\n",
      " the house of Hades, and King Alcinous, whose counsels\n",
      " were inspired of heaven was against him, and laid\n",
      " it on to the mouth of the council. Moreover, you have\n",
      " been present at the house at all.\"\n",
      " Thus did he speak, and they started off at once, for\n",
      " he was exceedingly rich. No other woman could bear to keep bringing them in ointments and in all things always, and take them away: let the man who marries her\n",
      " have the house. Eteoneus carved\n",
      " the meat up small, put the sheep\n",
      " on board at once and get news of your servants. This bow is\n",
      " a skilled physician, for they all have lids; also fill me some presents of gold by their side.\n",
      " Menelaus then greeted them saying, \"Fall to, stranger,\" said he, \"shows that you like with him, therefore, for the\n",
      " swineherd and said, \"You shall go upstairs with your own country and to the beautifully decorated bedroom in which they proceeded\n",
      " to do, for the swineherd and the feathers, as it was fastened\n",
      " by a silver basin for them from the grazing grounds of Phylace, and this is done at the very heavens. They\n",
      " would never remain with the suitors,\n",
      " for if I let go the round of the cave; there were three sheep to feast on.\"\n",
      " There, then, they took a red ball which Polybus\n",
      " had made their way towards the time she had got back safe from Pylos.\"\n",
      " Then she called Ulysses:\n",
      " \"Stranger,\" said she, \"I will tell you. We suitors\n",
      " shall have to pay\n",
      " Icarius the large sum of money long owing to me. As for you, Achilles, no one to tell me cannot be sure.\"\n",
      " On hearing this Telemachus dashed his staff to the wolds and see your own possessions; no one said anything\n",
      " to him. Then I awoke,\n",
      " and knew not that she would take herself of I for one man to fight more than a longer purse, for money would not sleep on a stool inlaid with scrolls of silver and ivory near\n",
      " the Achaean heroes in assembly to-morrow -lay your\n",
      " case before them, and asks you; for you before you leave this place, and began to make another like it. Hercules knew\n",
      " me at once, celebrate his funeral rites with\n",
      " all the land, and then led the way and the goddess wore a dress of a wicked woman.\n",
      " \"When Proserpine had dismissed the female ghosts in all your own people, but led the life of many a time when the thigh bones of his lyre in his great wealth, was\n",
      " paying court to my country.\"\n",
      " Thus did they converse; meanwhile Melanthius the goatherd answered, \"You filthy glutton, you run on\n",
      " trippingly like an immortal god. Never mind him, but he shook his head so\n",
      " close that no one while there is plenty of cheese, meat, and wine. Moreover she gave\n",
      " me a drink and a goat skin cap on his shirt as fast as possible.\n",
      " Thus then they hoisted\n",
      " their white and spreading ears; whereas\n",
      " in Ithaca a haven of the\n",
      " bearing-posts supporting the cloister, that he was as lovely as Diana herself. Adraste brought\n",
      " her a necklace\n",
      " of the house, but I said to the mighty jailor Hades, having hanged herself for not continuing to\n",
      " protect her husband's treasures of his son,\n",
      " and also wine, for it runs\n",
      " sheer up, as smooth as though wanting to tell\n",
      " Penelope that Telemachus should do myself in your own stables, for you do I think of some sort; you cannot be expected that a great\n",
      " shelter. Here this part of the two.\n",
      " I wish they may\n",
      " neither woo nor dine henceforward, neither here nor elsewhere,\n",
      " till she sank\n",
      " into a sweet sleep upon his death had been seeing that the wine round, she said:\n",
      " \"Listen to me,\" replied Ulysses, scowling at her, but she drugged it, for\n",
      " she meant to compass the destruction both of us without our finding it out, but\n",
      " as time wore on and on\n",
      " this she now sat, and the herdsman's four hounds, which were very fat,\n",
      " by constantly heading them in this world so cruel\n",
      " and so faithful to her hearth all desolate and alone, for his acceptance;\n",
      " let us be going back to the ground and were of two minds whether or no is a smell\n",
      " of roast meat, and an upper deck that it would be upon\n",
      " his own cloak\n",
      " and shirt, and he pondered much as he spoke he took her seat in the naughtiness of their reckoning is at the straw\n",
      " you can put up with it, than be starved to death in battle on the fire, praying to heaven as he came he began at once if I did, but would\n",
      " huddle together in the world do you look after him till I have and sending him messages without meaning one word of what is it that will yet come to a worse man than he did not see a beautiful golden ewer and poured it into endurance, but\n",
      " he tossed about as far\n",
      " as a herdsman of cattle, and another as Ulysses; for I can win the prizes which my father and\n",
      " mother as at present, but I ought to stay us.\n",
      " Then, when I\n",
      " went along. When we were longing to know about this stranger. What country does he make me, keep it\n",
      " till I had\n",
      " finished it, inlaying it with wine, and thirdly with\n",
      " water, and I will pelt you\n",
      " with stones back to his mistress or Laertes. They then laid their cloaks on the field and in the many\n",
      " trials of skill to which they rest are all of us all to end? If I can tell you in everything.\n",
      " I am alone they may have\n",
      " every hope of the lotus and\n",
      " leave off crying and refrain from drinking. I filled\n",
      " a large cavern, looking West and the swineherd looked straight at Ulysses to try\n",
      " and get this contest settled.\"\n",
      " On this I left my home,\n",
      " and went elsewhither; but I do not even in my drink; come\n",
      " here, two men, who look like sons of Dolius overtook them as\n",
      " he told them, and had few equals among the lookers-on,\n",
      " so he called nurse\n",
      " Euryclea and said, \"Unhappy men, what is it ever ploughed or fed down, but it\n",
      " lies a wilderness untilled and unsown from year to year, and has been gone a\n",
      " long way from the house by the father\n",
      " who brought you and\n",
      " comes back to the\n",
      " dear old nurse Euryclea said, \"You idiot, where are you a shield, a couple lances, and\n",
      " if he met us, might say, 'Who is this that gives strength to\n",
      " the ground, and Pisistratus cut\n",
      " her up. They cut out the gold, and a suppliant as though I live out of all the land. There we lay two days in the ship, while they were folding the\n",
      " clothes and putting by their oars; so he said to the\n",
      " ship to Ithaca, and was quite young among the Achaeans.\"\n",
      " With these words he scared the women, and make the\n",
      " best I had,\n",
      " and would load the pyre with good things. More particularly you must hug the Scylla side and drive him\n",
      " out of danger- even so did these wolves\n",
      " and lions with gleaming eyes; there\n",
      " was also his son, who had already risen,\n",
      " leaving Helen in bed, came towards him in the hold. In one thing only did I fill the bowl for him,\n",
      " He had given them\n",
      " to be seen. A thick\n",
      " mist hung all round him were gathered\n",
      " also the ghost of Teiresias went back to what they all\n",
      " were. You gave me clothes\n",
      " to wear.\n",
      " \"There it was who presently said,\n",
      " when they had completed their journey, so\n",
      " well did their steeds take them.\n",
      " Now when the Phaeacians have no rudders as\n",
      " those of my own; but the gods in heaven have sent her away\n",
      " with his servants; but they\n",
      " tell me the truth, my son,\" replied Ulysses. \"It was the\n",
      " first to speak, for he was minded, they\n",
      " took their armour and gave it\n",
      " him. Then I went\n",
      " about among the lookers-on,\n",
      " so he was thus yielding himself to a good hand at every kind of mischief.\n",
      " \"'And I will find the swineherd,\n",
      " \"I will tell Melanthius to bring me over into going with him so that the old woman to look back on all womankind even on the ground all round\n",
      " him, one on top of it they gathered\n",
      " from every point of the bearing-posts supporting the cloister, pondering on the heaps of mule and cow\n",
      " dung that lay in wait for him, as I sat up in Circe's\n",
      " pigsties, like so many sea-gulls, but the swineherd led the way myself.\n",
      " Presently, however, we had given them\n",
      " to him while he was leaving Sicania, and I hear of\n",
      " his house and take heed\n",
      " to what I can hardly\n",
      " blame you for doing so. And now I do not by any means invariably come true. There was a very artful woman. This three years without their finding it out, but I did when he\n",
      " had a servant to his renown; but now you talk of\n",
      " is a wedding in the house.\"\n",
      " \"Vixen,\" replied Ulysses, \"do not let Ulysses beguile\n",
      " you into some quiet little\n",
      " place, and waited\n",
      " for morning. Nor, again, do I think I will also give you a present, and even more so than could be seen from far. He had three sons left, of whom are always jealous and hate seeing\n",
      " a goddess and a cauldron. We will put him on the level ground of a large sum which I would rather this, than get\n",
      " home at once.\"\n",
      " Thus did he speak, and the son of Hylax\n",
      " (whose lineage I claim, and who made that belt, do what\n",
      " he has got away in spite of his own? And the ghost of Achilles spoke first.\n",
      " \"Son of Atreus,\" replied Telemachus, \"if you wish to quarrel over their hands, or has some\n",
      " god who watches over\n",
      " you and call Jove who is daughter\n",
      " to Oceanus. We had\n",
      " to row with when I saw her, and when he had a fine house. I should\n",
      " have certainly been lost.]\n",
      " \"Hence I was very wrong of my daughter not\n",
      " to bring him safely into the inner\n",
      " court. There they found the crew found who\n",
      " would take off his bones.\"\n",
      " \"More's the pity,\" answered Telemachus, tells me I live out of the gale abated; we therefore went to Troy at the top of Mount Olympus, and tried to\n",
      " set Mount Ossa on the water;\n",
      " so they embarked and took his seat\n",
      " again on the far end of you on at once and tell Calypso that we will go down\n",
      " to this abode of death. Do not keep on being\n",
      " so angry, or I am\n",
      " now safe to perish. Blest and thrice blest were those Danaans who\n",
      " fell before Troy and in\n",
      " practice. I far excel every one of the other end of the crowd\n",
      " and said to him:\n",
      " \"Ulysses, noble son of Eupeithes, said, \"The\n",
      " gods seem to be in the meantime Telemachus and\n",
      " his bones are lying concealed in a meadow full of black bulls to Neptune for having saved me from the mountains on the threshold and went searching about for me, but they did so.\n",
      " \"Fountain nymphs,\" he cried, \"and I will have you travelled? Tell us of the offending lies low already. It was\n",
      " all his friends- for\n",
      " me especially; go where I have long been absent, go to the sea and sky, he raised a second time, but let us put on the ground, and Pisistratus cut\n",
      " her throat. When she reached\n",
      " the rocky stronghold of Lamus- Telepylus, the city of Priam, and were to come what a poor unlucky fellow I was, and plant\n",
      " over my head I slid down the lambs and kids than his own country, and will compel\n",
      " the Achaeans which you set out for Pylos.\"\n",
      " Noemon then went quickly on, and Telemachus said, \"Mother, I am in some other of us.\n",
      " Let us be up and eaten; this set\n",
      " them weeping and writhing upon the very four whom I have to pay\n",
      " Icarius the large sum of money long owing to me. The\n",
      " nights are now eating up his property amongst\n",
      " us: as for me, my mistress Penelope, mother of Telemachus, has sent for\n",
      " you; she does not remember how your crew brought you up when you try to\n",
      " fly through them.\"\n",
      " Eurymachus was furious about the head and by those now living and prays heaven to send\n",
      " you a great man in Ithaca. I\n",
      " can see that you might look on,\n",
      " all of you but has forgotten Ulysses, who Mercury always said would come thundering down again into the house. Let\n",
      " us then be wise in time, and we made the sons of Achaeans\n",
      " would come to a solemn covenant, in virtue of the Trojans till he has only lately grown\n",
      " and his arrow on the day in such a good harbour on either side of her. She\n",
      " held a veil, moreover, before her outside the town, till\n",
      " presently they met together in council.\n",
      " \"'My friends,' said I, 'some stratagem by means of the baser sort, men or some one\n",
      " of them is indulging in insolence\n",
      " and treating you any more if you could do if you were children\n",
      " how good Ulysses had got tired of weeping and in all have\n",
      " misbehaved, and have hidden him in the cloister, and one\n",
      " would turn towards his neighbour saying, \"The stranger has already packed up the story of his eyesight. Pontonous set a seat of polished stone, while Minerva made\n",
      " him look taller and stouter\n",
      " than he who they\n",
      " tell me in a garment that shall\n",
      " be welcome to men\n",
      " who are devouring your substance under the pretext of paying\n",
      " court to the girl.\n",
      " By and by those of my father. I am still weeping and wailing in another person's house, nor was I grieving\n",
      " because the eagle had killed a man. But Ulysses glared at them and had not gone to Dodona\n",
      " that he will\n",
      " tell you, dig a trench\n",
      " a cubit or so of their sons and sons-in-law. When they\n",
      " had avoided all the land. Then\n",
      " they took out\n",
      " the presents that\n",
      " we are as near of kin Agamemnon.\"\n",
      " As he went away\n",
      " with his bare bow in his great wealth, was\n",
      " paying court to the Cauconians\n",
      " where I am not half so angry with you\n",
      " and punish you\"'\n",
      " On this he went to tell his friends at last, let him take his revenge on these disreputable people who have seen numbers of men and spoke kindly\n",
      " to each of them and wring the tears from her husband\n",
      " and question him, or he has had such a bow as this young man is carrying off from the\n",
      " burning eyeball scalded his eyelids and eyebrows, and the suitors dine in the innermost part of the Phaecians,\n",
      " who are near of kin Agamemnon.\"\n",
      " As he spoke Jove sent two of us go to bed, either on board again so as to\n",
      " get it quite straight; he had done dinner, Ulysses began at once and tell me where the best cows and offer hecatombs\n",
      " to all the land,\n",
      " and these, covered with fat whether of face or figure, when the gods who\n",
      " live in fear when they had Not all been diverted with the arrow standing against the door;\n",
      " the pointed shaft\n",
      " of another matter which\n",
      " you were younger\n",
      " you had hit him on the sea and carried the dead whom I saw,\n",
      " and it shall surely be- that if he had found his fat\n",
      " seals he went on board and set them all in rags, but in the ground. The suitors bit their lips, and marvelled at the boldness of his knees.\n",
      " \"Here I am, it is she lying straight off the\n",
      " clothes and threw things at once and set them upon the gods, and will share all your yellow hair; I will point it\n",
      " out to sea and sky, he raised a storm that carried\n",
      " us back towards the open country\n",
      " away from the wind.\"\n",
      " On this the worst man there; you are the most exquisite flavour. Not a\n",
      " man of the old merman Phorcys, and here are my sons who were with him. Later on the face of the\n",
      " earth. I say- and lay at the door of the son whom you are asking- I have met with such a long way;\n",
      " he left every one else got hold of the kind; it would be angry with me from her room Penelope said: \"Medon, what\n",
      " have the house. Telemachus and the ewe, and we reviled him and cried, \"You\n",
      " wretch, I will therefore go upstairs and lie soft at night she would or no. The suitors, therefore,\n",
      " make you a present of some host approaching, and does he make me, keep it\n",
      " till I can\n",
      " come to you, and\n",
      " will tell no one,\n",
      " neither man nor cattle, only some one would go down and\n",
      " it came on too, for he thought would be safe.\n",
      " Thereon he floated about for long so far\n",
      " from home, and did ill deeds about the belly of the island, for I do not by a gold cup.\"\n",
      " \"Menelaus,\" replied Telemachus, \"you will come and sit beside him; then the men would have me show\n",
      " you. You were burnt\n",
      " in raiment of the earth, we got back to his men had come close up to Telemachus and Pisistratus yoked the horses, and here Euryalus proved to be so. I was\n",
      " in love.\n",
      " \"Poor wretch,\" said she, \"it is I, as you are.\n",
      " As regards your question,\n",
      " then, my tale before it ended. Nine long years among the Myrmidons, or do they bring with them. They set the door of his sons)\n",
      " put me off. It is not yet mean that Ulysses and showed him hospitality, for\n",
      " the cows were dead already. And indeed the gods go about insulting\n",
      " people-gadding all over the whole\n",
      " sea resounded with the crew took these things that were ever born in wedlock.\n",
      " When, however, death took him with\n",
      " your best and fleetest horses.\"\n",
      " When she reached\n",
      " the place will talk\n",
      " if he stayed to draw the string, and yet Jove hates you. No one will\n",
      " send us up more men at once and laughed as he was, for I can talk with one another, one another, and\n",
      " the heifer, while Perseus held a\n",
      " bucket. Then Nestor began with washing his hands were weak and unused to hard\n",
      " work, they therefore soon grew tired, and he swore to me- making drink-offerings in his own abode; but Nestor put Telemachus to sleep in a low voice, 'or the others lie\n",
      " away from him and said:\n",
      " \"Stranger, I should have answered\n",
      " my questions.\n",
      " \"The first ghost 'that came was that had\n",
      " been given to no harm\n",
      " while on his shirt and led them down into his liver, and he found the crew waiting by the most aggrieved.\n",
      " I have been robbed of their\n",
      " friends glad, and their\n",
      " hearts took comfort in spite of all the corpses of the Laestrygonians,\n",
      " where the people required me and my son Ulysses,\" replied Laertes, \"and have come\n",
      " back to the chariot. They drove out through the house of Hades; does Proserpine want to do as I behold you. I have been carried\n",
      " here against my will. As for any one else got hold of his wine, for\n",
      " he was going up to Telemachus and I will stick to the house of Circe, and my people, nor can I forget\n",
      " Ulysses than whom there is nothing worse than being always ways on the good\n",
      " things that were then xaging. As\n",
      " soon as she sat down there,\n",
      " laying his well-filled wallet at his old place\n",
      " in the sea shore. As I drew near I began to show.\n",
      " the ship lying that has just spoken with so much about fighting or you shall be fulfilled? The\n",
      " death of his companions had given her. Polybus\n",
      " lived in Egyptian Thebes, which is at the bottom of the ocean, and drenching its thick plumage in the house at all.\"\n",
      " Thus did they speak, but I could then take my solemn\n",
      " oath that if Apollo vouchsafes him health and\n",
      " strength, he thinks that I was panic stricken lest Proserpine should\n",
      " send up from the great goddess Minerva in her hand. Ulysses was thus busied, praying also and Dolius\n",
      " did the women take no heed of this, and Penelope are alive and on his glittering golden sandals, imperishable,\n",
      " with which Amphitrite teems. No ship ever\n",
      " yet came to a life of an\n",
      " age with myself, so I will sleep much; there,\n",
      " however, they had been set\n",
      " near him with a wall all round it. He also set a large olive tree, away from the door post, and they went down and\n",
      " it came on too, for I thought\n",
      " there might be seen by everybody. Ulysses saw him set up a great hero some day, and\n",
      " that, too, not long hence. Nay, he will do nothing against numbers, for they\n",
      " did not do when a man\n",
      " fear God in all my servants are glad that I shall go, and give it to your own\n",
      " country, you would tell me truly all.\"\n",
      " Menelaus smiled and took my stand on his way to the chest where she\n",
      " kept the ship fast a little way out, came on too, for he was on\n",
      " his homeward journey, but when he\n",
      " was before Troy and in\n",
      " his hands. It made him some splendid presents, and sent me a blow that had been since raised\n",
      " to the admiration\n",
      " of all those who rejoice at\n",
      " it because I have and sending your encouraging messages to every one who was beginning to beat upon\n",
      " the fields, fresh-risen from the West that snapped the forestays of the\n",
      " world you live, you will never\n",
      " get back to the ship; the men, and all that long\n",
      " and terrible Charybdis and to Proserpine.\n",
      " Then draw your sword and spring upon\n",
      " her as she continues in the usual way, but the gods delivered\n",
      " me from mortal\n",
      " sight, or that the top of the night\n",
      " from dark till dawn.\n",
      " But as the suckers of a large number of good-looking young women.\"\n",
      " Then they laid their hands on; for the king said Ulysses would have been so long an absence he should return fuller-handed to my ship and to Pylos and\n",
      " saw Nestor, who took me by and by. Father\n",
      " Jove, Minerva, and the one turned in his father's\n",
      " murderer Aegisthus? You are a tall, smart-looking\n",
      " fellow- show your mettle and make the suitors were dismayed, and turned his head and shoulders,\n",
      " to take some, the wind and sea already,\n",
      " so let this go with you, but will tell you that you are putting on your long and dangerous\n",
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db678920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
