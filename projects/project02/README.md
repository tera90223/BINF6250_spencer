# Introduction
Script for project 2 is capable of creating a markov model of Nth order given a piece of text, and calculate the transition probabilities between states. Using these probabilities, new text is able to be generated by stochastic selection of all possible next states, in this case, individual words. 

# Pseudocode
Building Markov Model: 
if we're just starting, our model should be empty
check if the input markov model is empty:
|--> if yes, set a flag is_start to true
|--> if no, say that the previous word was the start state

check if the line we're looking at is an empty string ""
|--> if yes, we're at the end of the file and are going to the end state, set a flag is_end to true
|--> if not, nothing happens

now pass the is_start and is_end flags to this function:


First function: build_markov_model (not paramaterizing the order yet)
Takes as input: 
- a Markov model (dict of dicts) (does NOT have to be empty)
- a string of text (expect this to be an entire line from a work of literature)
- a list of the last N words in the previous line , where N is our order
- boolean indicating whether this is the first line of the file (is_start)
- boolean indicating whether this is the end of the file (is_end)

This function trains (adds to) a 0-order Markov model based on the inputted text
Needs to NOT overwrite what's already in the model

Here's the data structure that the Markov model uses to store its edges and frequencies: {str: {str: int}}


At the start of working with this line, do all this:

now we can split the line into separate words--> treat punctuation as part of a word

Here's our scheme for how we handle the start and end states in the model: 

*S* --> line                    happens at start of file
\n from prev --> line           happens for all middle lines
\n from prev --> *E*            happens after the last line of text

iterate over the words' indices such that i+1 will always be in the list (go from index 0 to length-1)

Call the helper function below, passing it word 1 and word 2
|--> we can use word i for word 1 or overwrite it with whatever the start state or the previous word was from the previous line (because we have that in our arguments)
|--> we can use word i+1 as word 2 or overwite it with the end state if we're at the end of the file

Now that we've broken from the loop, we can return the updated dictionary of dicts
make a list of the last N words in the line, where N is our order + 1) and return that too


Make this a helper function that takes the dict of dicts, word i, and word i+1 as inputs:
    access word i and word i+1
    go into the outer dictionary and try to access the key for word i using dict.get()
    |--> if it doesn't exit, pass dict.get() the default value of an empty dict
    
Now the inner dict definitely exists but may not have the key

access the inner dictionary and attempt to access the key for word i+1 using dict.get()
    |--> if it doesn't exist set the default to 0
    set the value for this key in the inner dictionary to be 1 if it wasn't already in the dict, and add 1 to it if it already exists

return the updated dictionary
    So this function gets called by assigning the dict of dicts to the value that this returns

Here is how we planned to access the outter and inner dictionaries in our model while updating counts (this method let us access a key's value or assign a key a default value if the key wasn't already in the dict. 
dict.get(key, default_value)
dict[key] = dict.get(key, default_value={})
dict[word2] = dict.get(word2, default_value=0) + 1

Text Generation: 
Calculate transition probilities for all next states from a given state (counts/sum)
    Initialize a probability dictionary
    Access the value in the outer dict associated with our current word
    Once there, we just sum all of the values in current word's inner dictionary
    And then for each word that's a key in the inner dict, we take its count and divide by the sum
    Now in the probability dict, map that key from the inner dict to the number we just didvided for it <3
    Randomly draw from these to generate the next state
    Just pass the values from the probability dict to np.random.choice


# Successes
Some successes worth highlighting: 
The ability of the group to work together, bulk of code was written during open dialogue sessions, where ideas were floated and implemented. Additionally, using in-class feedback about the projects, we were able to implement more functions than the ones that were 'required', adhering as closely as possible to the one function -> one purpose guidline. 

# Struggles
We were unaware that when numpy randomly chose a string from our dictionary of probabiilities, it would return a numpy string. This broke the comparison between the keys in the dictionary (which were normal strings) and the tuple containing the last N randomly selected words (which got returned as numpy strings) and caused our model to fail to generate anything. 
Our choice to have the end state only come after the end of the file (as opposed to at the end of each line or some other alternative), combined with the fact that each of these models is only trained on one file, meant that our model could only hit an end state and organically finish generating text when it generated the specific word or sequence of words that happened to be at the end of the training file. This led to our model generating large amounts of text until it would stumble into the only word or sequence of words that allowed it to break from its infinite loop, or (more realistically) it would run out of memory. The solution to this was a tweak that allowed the model to stop generating text if it generated a sequence of words that didn't exist in the training data (such a sequence would have no edges in our Markov model). 

# Personal Reflections
## Group Leader
Spencer: I appreciated working with both Chantera and Justin for this project as each posses strong critical thinking skills that facilitated the group towards implementing code that solved the problems at hand, and was backed up by well thought out planning. Every member of the team was flexible with their schedules, and we were able to all work on this project together, which I believed strengthened the final product.   

## Other member
Chantera: I enjoyed working with both Spencer and Justin. We were able to coordinate our schedule and truly be collaborative. We all took time outside of class to plan and code together, instead of the typical divide and conquer, and this allowed us to learn together and exchange ideas more effortlessly. When one of us had questions, the others in the group made sure to clear it up to ensure everyone was on the same page. 

Justin: I am happy with how it went working with Chantera and Spencer. We were all able to coordinate to meet up multiple times outside of class and work in tandem over a teams meeting and discuss our perspectives on how to tackle the issues. I did my best to take initiative with starting the pseudocode and scripting in python to get the ball rolling at the start of our meetings, and I am glad that I was able to jumpstart our sessions and get discussion flowing by doing that. We did a great job of bouncing ideas off of each other and working through everything together, which is a lot more enjoyable than trying to divide and conquer. 

# Generative AI Appendix
We did not use generative AI during this project. 
