{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af348eba-b1ca-4b2b-a49a-ecd0cd2de78a",
   "metadata": {},
   "source": [
    "# Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3de6f-a30a-4a2e-91f4-593f1f41d88a",
   "metadata": {},
   "source": [
    "---\n",
    "In class today we will be implementing a Markov chain to process sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6dda45-e3c8-43b4-8362-e43292a3c6c3",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "\n",
    "1. Students will be able to explain the Markov Chain process\n",
    "1. Implement a Markov Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4394c-9c65-4ed4-bfe8-0db56ec32223",
   "metadata": {},
   "source": [
    "Markov Chains represent a series of events following the Markov Property: future states are memory-less in that they depend only on the current state. This can be expanded to the idea of variable order Markov models where there is a variable-length memory (eg. 1st order Markov Model). Markov models consist of fully observable states. \n",
    "\n",
    "> A common example of this is in predicting the weather: We can clearly see the current weather and would like to predict tomorrow's weather. This is also applicable to biology with one case being CpG islands. \n",
    "\n",
    "Our goal today will be to implement a Markov model built from words. For our example text, we will use the classic example of Dr. Seuss because of the repetitive nature of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69269a05-73d5-47e4-96f3-29e25ed3b928",
   "metadata": {},
   "source": [
    "---\n",
    "## Train Markov model\n",
    "\n",
    "For our initial implementation of the Markov Model, we will use the simple example of Dr. Seuss: \"One fish two fish red fish blue fish.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2e7a1-b676-4874-ad72-fe14edf98a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entry(markov_model: dict, word_one: str, word_two: str):\n",
    "    \"\"\"\n",
    "    Function to access the markov model and add 1 to word 1's inner dictionary entry for word 2\n",
    "    This adds 1 to the count of times where word 1 comes before word 2 in the text\n",
    "    \n",
    "    @param markov_model: dictionary of dictionaries (may be empty) mapping strings to a dict of {string: integer}\n",
    "    @param word_one: str, it's the word being used as the key to the outer dictionary\n",
    "    @param word_two: str, the word being used as the key to the inner dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # access the outer dictionary's value for word 1 (set it to a blank dictionary if the key doesn't exist)\n",
    "    markov_model[word_one] = markov_model.get(word_one, {})\n",
    "\n",
    "    # access the inner dictionary and attempt to access the key for word two (and set the count to 0)\n",
    "    # regardless of if the value existed before, we're adding 1 to it\n",
    "    markov_model[word_one][word_two] = markov_model[word_one].get(word_two, 0) + 1 # I SWEAR this isn't as ratchet as it looks!!!!\n",
    "\n",
    "    # now return the updated model\n",
    "    return markov_model\n",
    "\n",
    "\n",
    "def build_markov_model(markov_model: dict, new_text: str):\n",
    "    '''\n",
    "    Function to build or add to a 1st order Markov model given a string of text\n",
    "    We will store the markov model as a dictionary of dictionaries\n",
    "    The key in the outer dictionary represents the current state\n",
    "    and the inner dictionary represents the next state with their contents containing\n",
    "    the transition probabilities.\n",
    "    Note: This would be easier to read if we were to build a class representation\n",
    "           of the model rather than a dictionary of dictionaries, but for simplicitiy\n",
    "           our implementation will just use this structure.\n",
    "    \n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "        new_text (str): a string to build or add to the moarkov_model\n",
    "\n",
    "    Returns:\n",
    "        markov_model (dict of dicts): an updated markov_model\n",
    "        \n",
    "    Pseudocode:\n",
    "        Add artificial states for start and end\n",
    "        For each word in text:\n",
    "            Increment markov_model[word][next_word]\n",
    "        \n",
    "    '''\n",
    "\n",
    "    # split the line into separate words\n",
    "    sentence = new_text.split()\n",
    "\n",
    "    # adds the entry for the start state transitioning to the first word in the sentence\n",
    "    add_entry(markov_model, word_one = \"*S*\", word_two = sentence[0])\n",
    "\n",
    "    # iterate over the indices of the words in the sentence\n",
    "    for i in range(len(sentence) - 1):\n",
    "        # add the entry for word i transitioning to word i+1\n",
    "        add_entry(markov_model, word_one = sentence[i], word_two = sentence[i+1])\n",
    "    \n",
    "    # now that we're done with the words in the sentence, we just need to add the end state\n",
    "    add_entry(markov_model, word_one = sentence[-1], word_two = \"*E*\")\n",
    "\n",
    "    # return the trained model\n",
    "    return markov_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc64609-ae18-4d8e-acd9-0e6a6110bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'*S*': {'one': 1}, 'one': {'fish': 1}, 'fish': {'two': 1, 'red': 1, 'blue': 1, '*E*': 1}, 'two': {'fish': 1}, 'red': {'fish': 1}, 'blue': {'fish': 1}}\n"
     ]
    }
   ],
   "source": [
    "markov_model = dict()\n",
    "text = \"one fish two fish red fish blue fish\"\n",
    "markov_model = build_markov_model(markov_model, text)\n",
    "print (markov_model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7d42532-a75f-4ee4-aad0-fb9d6843050c",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "{'*S*': {'one': 1}, 'one': {'fish': 1}, 'fish': {'two': 1, 'red': 1, 'blue': 1, '*E*': 1}, 'two': {'fish': 1}, 'red': {'fish': 1}, 'blue': {'fish': 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36386041-90a1-4c7a-967c-ed08c38f56c0",
   "metadata": {},
   "source": [
    "###  Nth order Markov chain\n",
    "In the above model, each event or word is output from only the previous state with no memory of any prior states. While this is useful in some cases, typical biological applications of Markov chains require higher-order models to accurately capture what we know about a system. For instance, in attempting to identify coding regions of a genome, we know that open reading frames (ORFs) contain codon triplets, and so a third or sixth order Markov chain would better describe these regions. Here you will implement a generalized form of our previous Markov Chain to allow for Nth order chains.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "796cf4ce-29e9-4507-a7e0-afc0e9b1429c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T17:52:04.016115Z",
     "start_time": "2026-02-02T17:52:04.003656Z"
    }
   },
   "source": [
    "def add_multi(markov_model: dict, key_tuple: tuple[str], next_word: str):\n",
    "    \"\"\"\n",
    "    Function to access the markov model and add 1 to word 1's inner dictionary entry for word 2\n",
    "    This adds 1 to the count of times where word 1 comes before word 2 in the text\n",
    "    \n",
    "    @param markov_model: dictionary of dictionaries (may be empty) mapping strings to a dict of {string: integer}\n",
    "    @param key_tuple: tuple of strings (variable length), it's the sequences of words being used as the key to the outer dictionary\n",
    "    @param next_word: str, the word being used as the key to the inner dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # access the outer dictionary's value for word 1 (set it to a blank dictionary if the key doesn't exist)\n",
    "    markov_model[key_tuple] = markov_model.get(key_tuple, {})\n",
    "\n",
    "    # access the inner dictionary and attempt to access the key for word two (and set the count to 0)\n",
    "    # regardless of if the value existed before, we're adding 1 to it\n",
    "    markov_model[key_tuple][next_word] = markov_model[key_tuple].get(next_word, 0) + 1 # I SWEAR this isn't as ratchet as it looks!!!!\n",
    "\n",
    "    # now return the updated model\n",
    "    return markov_model\n",
    "\n",
    "\n",
    "def build_markov_model(markov_model, text, order=1):\n",
    "    '''\n",
    "    Function to build or add to a Nth order Markov model given a string of text\n",
    "    Still only takes one line of text as input\n",
    "    dict mapping tuples of N strings to dictionaries that map strings to ints\n",
    "    {tuple(str): {str: int}}\n",
    "    where N is the order\n",
    "\n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "            or None if a new model is being built\n",
    "        new_text (str): a string to build or add to the moarkov_model\n",
    "        order (int): the number of previous states to consider for the model\n",
    "        \n",
    "    Returns:\n",
    "        markov_model (dict of dicts): an updated/new markov_model\n",
    "    '''\n",
    "\n",
    "    # split the line of text into a list of words\n",
    "    sentence = text.split(\" \")\n",
    "\n",
    "    # check if our order is greater than the length of the line\n",
    "    if order > len(sentence):\n",
    "        # print the problem to stdout and return the still-untrained model\n",
    "        print(f\"The order {order} is too high for the line length of {len(sentence)} words! Quitting without training the model...\")\n",
    "        return markov_model\n",
    "\n",
    "    # since we know we're at the start of the line, map the start state to the first word and then slowly flush out the start states\n",
    "    for i in range(order):\n",
    "        # we need a decreasing number of start states in our tuple of the outter dictionary\n",
    "\n",
    "        # make a little list contianing all the instances of the start state that we need for whatever iteration we're currently on\n",
    "        start_list = [\"*S*\"] * (order - i)\n",
    "        # print(f\"Start list: {start_list}\")\n",
    "\n",
    "        # iterate over the words in the sentence that we have already hit\n",
    "        for word in sentence[:i]:\n",
    "            # add the words we've already hit to the key\n",
    "            start_list.append(word)\n",
    "        # print(f\"after adding words: {start_list}\")\n",
    "\n",
    "        # now convert the key, currently a list, to a tuple\n",
    "        key = tuple(start_list)\n",
    "\n",
    "        # add this key to the markov model\n",
    "        markov_model[key] = markov_model.get(key, {})\n",
    "\n",
    "        # add entry to the markov model mapping this key to the next word in the sentence\n",
    "        add_multi(markov_model, key, sentence[i])\n",
    "\n",
    "    # now that we're out of that for loop, we can make our keys entirely out of words that are in the list\n",
    "    # iterate over the indices of the words in the list\n",
    "    for j in range(len(sentence) - order):\n",
    "        # prepare the tuple that we'll use as a key, which includes N words from the line\n",
    "        key = tuple(sentence[j:j+order])\n",
    "        # print(f\"key: {key} // value: {sentence[j+order]}\")\n",
    "\n",
    "        # add an entry mapping the current N words in the window to the next word after\n",
    "        add_multi(markov_model, key, sentence[j+order])\n",
    "\n",
    "    # now we are missing the last entry in our dict that maps the last N words to the end state\n",
    "    key = tuple(sentence[-1 * order:])\n",
    "    add_multi(markov_model, key, \"*E*\")\n",
    "\n",
    "    # return the updated markov model\n",
    "    return markov_model\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "62b8f110-6cad-41bb-b976-261a136282ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T17:52:10.397284Z",
     "start_time": "2026-02-02T17:52:10.364779Z"
    }
   },
   "source": [
    "markov_model = dict()\n",
    "text = \"one fish two fish red fish blue red fish blue\"\n",
    "markov_model = build_markov_model(markov_model, text, order=2)\n",
    "markov_model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('*S*', '*S*'): {'one': 1},\n",
       " ('*S*', 'one'): {'fish': 1},\n",
       " ('one', 'fish'): {'two': 1},\n",
       " ('fish', 'two'): {'fish': 1},\n",
       " ('two', 'fish'): {'red': 1},\n",
       " ('fish', 'red'): {'fish': 1},\n",
       " ('red', 'fish'): {'blue': 2},\n",
       " ('fish', 'blue'): {'red': 1, '*E*': 1},\n",
       " ('blue', 'red'): {'fish': 1}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "raw",
   "id": "0bf821ed-d84f-4fe7-9de9-3c21a3841aa3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Expected output:\n",
    "{('*S*', '*S*'): {'one': 1},\n",
    " ('*S*', 'one'): {'fish': 1},\n",
    " ('one', 'fish'): {'two': 1},\n",
    " ('fish', 'two'): {'fish': 1},\n",
    " ('two', 'fish'): {'red': 1},\n",
    " ('fish', 'red'): {'fish': 1},\n",
    " ('red', 'fish'): {'blue': 1},\n",
    " ('fish', 'blue'): {'fish': 1},\n",
    " ('blue', 'fish'): {'*E*': 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14699827-2b59-4490-a961-a9ed04351a8e",
   "metadata": {},
   "source": [
    "## Generate text from Markov Model\n",
    "\n",
    "Markov models are \"generative models\". That is, the probability states in the model can be used to generate output following the conditional probabilities in the model.\n",
    "\n",
    "We will now generate a sequence of text from the Markov model. For this section, I recommend using np.random.choice, which allows for you to provide a probability distribution for drawing the next edge in the chain."
   ]
  },
  {
   "cell_type": "code",
   "id": "baea1038-ff96-46d4-b61a-cbd0777263f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T18:34:39.006475Z",
     "start_time": "2026-02-02T18:34:38.992355Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_probabilities(word_counts: dict[str, int]):\n",
    "    \"\"\"\n",
    "    Function to calculate the frequency of words, given a dictionary indicating how many times each word occurs\n",
    "\n",
    "    Frequencies get calculated by summing all the counts in the dict and dividing each count by that sum\n",
    "\n",
    "    @param word_counts: dictionary mapping strings to integers, the ints are the counts of how many times each of those strings occurs\n",
    "    @return: dictionary mapping strings to floats, where each float is the frequency (decimal form) of the word\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate the sum of all the words' counts\n",
    "    total = sum(word_counts.values())\n",
    "\n",
    "    # initialize the frequency table\n",
    "    frequencies = {}\n",
    "\n",
    "    # now iterate over the key-value pairs in the input dictionary\n",
    "    for word, count in word_counts.items():\n",
    "        # get the frequency of this word and store it in the dict\n",
    "        frequencies[word] = count/total\n",
    "\n",
    "    return frequencies\n",
    "\n",
    "\n",
    "def get_next_word(current_word, markov_model, seed=42):\n",
    "    '''\n",
    "    Function to randomly move a valid next state given a markov model\n",
    "    and a current state (word)\n",
    "    \n",
    "    Args: \n",
    "        current_word (tuple): a word that exists in our model\n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "\n",
    "    Returns:\n",
    "        next_word (str): a randomly selected next word based on transition probabilies\n",
    "        \n",
    "    Pseudocode:\n",
    "        Calculate transition probilities for all next states from a given state (counts/sum)\n",
    "            Initialize a probability dictionary\n",
    "            Access the value in the outer dict associated with our current word\n",
    "            Once there, we just sum all of the values in current word's inner dictionary\n",
    "            And then for each word that's a key in the inner dict, we take its count and divide by the sum\n",
    "            Now in the probability dict, map that key from the inner dict to the number we just didvided for it <3\n",
    "        Randomly draw from these to generate the next state\n",
    "            Just pass the values from the probability dict to np.random.choice\n",
    "        \n",
    "    '''\n",
    "    # access the part of the markov model with all the edges that come from the current word\n",
    "    if current_word not in markov_model:\n",
    "        return None\n",
    "\n",
    "    edge_counts = markov_model[current_word]\n",
    "\n",
    "\n",
    "    # calculate the probabilities for each of the edges\n",
    "    probability_dict = calculate_probabilities(edge_counts)\n",
    "\n",
    "    # randomly select words from the probability dict\n",
    "    next_word = str(np.random.choice(a=list(probability_dict.keys()), p=list(probability_dict.values())))\n",
    "\n",
    "    # return the word\n",
    "    return next_word\n",
    "\n",
    "def generate_random_text(markov_model, seed=42):\n",
    "    '''\n",
    "    Function to generate text given a markov model\n",
    "    \n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "\n",
    "    Returns:\n",
    "        sentence (str): a randomly generated sequence given the model\n",
    "        \n",
    "    Pseudocode:\n",
    "        Initialize sentence at start state\n",
    "        Until End State:\n",
    "            append get_next_word(current_word, markov_model)\n",
    "        Return sentence\n",
    "        \n",
    "    '''\n",
    "    # initialize our output string\n",
    "    generated_text = []\n",
    "\n",
    "    # start at the start state\n",
    "    curr_word = \"*S*\"\n",
    "\n",
    "    # start a loop that runs until we hit the end state\n",
    "    while curr_word != \"*E*\":\n",
    "        # append the current word\n",
    "        generated_text.append(curr_word)\n",
    "\n",
    "        # switch to the next word\n",
    "        curr_word = get_next_word(curr_word, markov_model, seed)\n",
    "\n",
    "    # now that we've broken from the loop, we need to append the end state\n",
    "    generated_text.append(curr_word)\n",
    "    \n",
    "    # now join all the text and ship it\n",
    "    return \" \".join(generated_text)"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "4b8724eb-5e10-4ffc-a76e-8a2015b3ee47",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9280c1d-9f9e-47d8-8697-cc6255b3ec7d",
   "metadata": {},
   "source": [
    "## All the Fish\n",
    "Up till now, you have only been working with a line or two of the Dr. Seuss' _One Fish, Two Fish_. Now, I want you to build a model using the whole book and try different orders of Markov models."
   ]
  },
  {
   "cell_type": "code",
   "id": "c04a137e-4c1d-40df-9056-177c022009ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T18:34:44.376466Z",
     "start_time": "2026-02-02T18:34:44.346113Z"
    }
   },
   "source": [
    "\n",
    "def update_markov_model(markov_model, text, order: int = 1, prev_line: str = None):\n",
    "    '''\n",
    "    Function to build or add to a Nth order Markov model given a string of text\n",
    "    dict mapping tuples of N strings to dictionaries that map strings to ints\n",
    "    {tuple(str): {str: int}}\n",
    "    where N is the order\n",
    "\n",
    "    This assumes that there will be more lines after the one that this reads\n",
    "    This is resilient against not knowing whether the current line is the first in the file or if it's not\n",
    "\n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "            or None if a new model is being built\n",
    "        text (str): a string to build or add to the moarkov_model\n",
    "        order (int): the number of previous states to consider for the model\n",
    "        prev_line (str): the previous line of text (optional)\n",
    "        \n",
    "    Returns:\n",
    "        markov_model (dict of dicts): an updated/new markov_model\n",
    "        tuple of the last N words in the line so the next line isn't treated as the start of the file\n",
    "    '''\n",
    "\n",
    "    # split the line of text into a list of words\n",
    "    sentence = text.split(\" \")\n",
    "\n",
    "    # check if our order is greater than the length of the line\n",
    "    if order > len(sentence):\n",
    "        # print the problem to stdout and return the still-untrained model\n",
    "        print(sentence)\n",
    "        print(f\"The order {order} is too high for the line length of {len(sentence)} words! Quitting without training the model...\")\n",
    "        return markov_model\n",
    "\n",
    "    # since we know we're at the start of the line, map the start state to the first word and then slowly flush out the start states\n",
    "    for i in range(order):\n",
    "        # we need a decreasing number of start states in our tuple of the outter dictionary\n",
    "\n",
    "        # check if we have a previous line to pull from\n",
    "        if prev_line:\n",
    "            # print(f\"HEY PREV_LINE WAS TRUE BTW AT I = {i}\")\n",
    "\n",
    "            # take a slice of the last order - i words and make that a list\n",
    "            start_list = prev_line.split(\" \")[-1 * (order - i):]\n",
    "\n",
    "        else:\n",
    "            # since we don't have a previous line to pull from, just use the start state (we're at the start of the file)\n",
    "            # make a little list contianing all the instances of the start state that we need for whatever iteration we're currently on\n",
    "            start_list = [\"*S*\"] * (order - i)\n",
    "            # print(f\"Start list: {start_list}\")\n",
    "\n",
    "        # iterate over the words in the sentence that we have already hit\n",
    "        for word in sentence[:i]:\n",
    "            # add the words we've already hit to the key\n",
    "            start_list.append(word)\n",
    "        # print(f\"after adding words: {start_list}\")\n",
    "\n",
    "        # now convert the key, currently a list, to a tuple\n",
    "        key = tuple(start_list)\n",
    "        # print(f\"Key: {key}\")\n",
    "        \n",
    "\n",
    "        # add this key to the markov model\n",
    "        markov_model[key] = markov_model.get(key, {})\n",
    "\n",
    "        # add entry to the markov model mapping this key to the next word in the sentence\n",
    "        add_multi(markov_model, key, sentence[i])\n",
    "\n",
    "    # now that we're out of that for loop, we can make our keys entirely out of words that are in the list\n",
    "    # iterate over the indices of the words in the list\n",
    "    for j in range(len(sentence) - order):\n",
    "        # prepare the tuple that we'll use as a key, which includes N words from the line\n",
    "        key = tuple(sentence[j:j+order])\n",
    "        # print(f\"key: {key} // value: {sentence[j+order]}\")\n",
    "\n",
    "        # add an entry mapping the current N words in the window to the next word after\n",
    "        add_multi(markov_model, key, sentence[j+order])\n",
    "\n",
    "    # now we are missing the last entry in our dict that maps the last N words to the end state\n",
    "    # key = tuple(sentence[-1 * order:])\n",
    "    # add_multi(markov_model, key, \"*E*\")\n",
    "\n",
    "    # return the updated markov model\n",
    "    return markov_model\n",
    "\n",
    "\n",
    "def train_model(model: dict, path: str, order: int = 1):\n",
    "    \"\"\"\n",
    "    Function to train a markov model off of the supplied text file\n",
    "    Takes a dict as input so you can update a model based on another file without overwriting it\n",
    "\n",
    "    @param model: dict if empty, or dict mapping tuples to dictionaries of string:int\n",
    "    @param path: str, the relative path to the text file the model is being trained on\n",
    "    @param order: int, the order of the Markov model (i.e. how many words long is the key in the outer dict)\n",
    "    @return: a trained markov model, a dict of dicts, where the key to the outer dict is a tuple of strings \n",
    "             and the inner dict maps strings to ints\n",
    "    \"\"\"\n",
    "    # Initialize the previous line\n",
    "    prev_line = None\n",
    "\n",
    "    # open the file\n",
    "    with open(path, mode=\"r\", encoding=\"utf-8\") as book:\n",
    "        # iterate over the lines of the file\n",
    "        for line in book:\n",
    "            # add the data from the \n",
    "            trained_model = update_markov_model(model, line, order, prev_line)\n",
    "            \n",
    "            # update the previous line before moving to the next\n",
    "            prev_line=line\n",
    "\n",
    "    # send the model to the end state\n",
    "    trained_model = update_markov_model(model, \"*E*\", order, prev_line=prev_line)\n",
    "\n",
    "    # pass the trained model\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "def generate_text(markov_model, order: int = 1, seed: int = 42):\n",
    "    '''\n",
    "    Function to generate text given a markov model\n",
    "    Why does this exist and how is it different from generate_random_text()?\n",
    "        *S* and *E* need to be tuples for our parameterized\n",
    "    \n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "        order: (int) an int indicating what order of markov model we're working with\n",
    "\n",
    "    Returns:\n",
    "        sentence (str): a randomly generated sequence given the model\n",
    "        \n",
    "    Pseudocode:\n",
    "        Initialize sentence at start state\n",
    "        Until End State:\n",
    "            append get_next_word(current_word, markov_model)\n",
    "        Return sentence\n",
    "        \n",
    "    '''\n",
    "    # initialize our output string\n",
    "    generated_text = []\n",
    "\n",
    "    # initialize start state\n",
    "    curr_state = [\"*S*\"] * order\n",
    "    curr_word = None\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # access whatever the next word is and add it to our generated text\n",
    "        #print(f\"Current state: {curr_state}\")\n",
    "        curr_key = tuple(curr_state)\n",
    "        #print(f\"Current key: {curr_key} // type: {type(curr_key)}\")\n",
    "        curr_word = get_next_word(curr_key, markov_model, seed)\n",
    "        if curr_word is None or curr_word == \"*E*\":\n",
    "            break\n",
    "        generated_text.append(curr_word)\n",
    "\n",
    "        # remove the 0th item in the curr_state list and add the next word to the end of the current state\n",
    "        del curr_state[0]\n",
    "        curr_state.append(curr_word)\n",
    "    \n",
    "    # now join all the text and ship it\n",
    "    return \" \".join(generated_text)\n",
    "\n",
    "\n",
    "\n",
    "# Now just add some more training data to the markov model. You can find it under data/one_fish_two_fish.txt\n",
    "\n",
    "markov_model = dict()\n",
    "# Read in the whole book\n",
    "text_file = \"data/one_fish_two_fish.txt\"\n",
    "first_order = train_model(model=dict(), path=text_file, order=3)\n",
    "#print(first_order)\n",
    "#for key,val in first_order.items():\n",
    "#    print(f\"Key: {key} // Type: {type(key)} // Value: {val} // Type: {type(val)}\")\n",
    "#print(f\"Markov model: {type(first_order)}\")\n",
    "print (generate_text(first_order, order=3, seed=7))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SO...\\n']\n",
      "The order 3 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['SO...\\n']\n",
      "The order 3 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['So...\\n']\n",
      "The order 3 is too high for the line length of 1 words! Quitting without training the model...\n",
      "['*E*']\n",
      "The order 3 is too high for the line length of 1 words! Quitting without training the model...\n",
      "One fish, Two fish, Red fish, Blue fish,\n",
      " Black fish, Blue fish, Old fish, New fish.\n",
      " This one has a little star.\n",
      " Say! What a lot of funny things go by.\n",
      " Some have two feet and some have four.\n",
      " Some have six feet and some have more.\n",
      " Where do they come from? I can't say.\n",
      " But I bet they have come a long, long way.\n",
      " we see them go.\n",
      " Some are fast. Some are slow.\n",
      " Some are high. Some are low.\n",
      " Not one of them is like another.\n",
      " Don't ask us why, go ask your mother.\n",
      " Say! Look at his fingers!\n",
      " One, two, three...\n",
      " How many fingers do I see?\n",
      " One, two, three, four,\n",
      " five, six, seven, eight, nine, ten.\n",
      " He has eleven!\n",
      " Eleven! This is something new.\n",
      " I wish I had eleven too!\n",
      " Bump! Bump! Bump!\n",
      " Did you ever fly a kite in bed?\n",
      " did you ever walk with ten cats on your head?\n",
      " Did you ever fly a kite in bed?\n",
      " did you ever walk with ten cats on your head?\n",
      " Did you ever milk this kind of cow?\n",
      " Well, we can do it. We know how.\n",
      " If you never did, you should.\n",
      " These things are fun, and fun is good.\n",
      " Hello, hello. Are you there?\n",
      " Hello! I called you up to say hello.\n",
      " I said Hello.\n",
      " Can you hear me, Joe?\n",
      " Oh no, I cannot hear your call at all.\n",
      " This is not good, and I know why.\n",
      " A mouse has cut the wire, goodbye!\n",
      " From near to far, from here to there,\n",
      " Funny things are everywhere.\n",
      " These yellow pets are called the Zeds.\n",
      " They have one hair upon their heads.\n",
      " Their hair grows fast. So fast they say,\n",
      " They need a haircut every day.\n",
      " Who am I? My name is Ned\n",
      " I do not like this bed at all.\n",
      " a lot of ink,\n",
      " you should get a Yink, I think.\n",
      " Hop, hop, hop! I am a Yop\n",
      " All I like to do is hop,\n",
      " From finger top to finger top.\n",
      " I hop from left to right and then...\n",
      " Hop, hop! I hop right back again.\n",
      " I like to hop all day and night.\n",
      " From right to left and left to right.\n",
      " Why do I like to box. How I like to have him in the house.\n",
      " At our house we play out back.\n",
      " We play a game called ring the Gack.\n",
      " Would you like to play this game?\n",
      " Come down! We have the only Gack in town.\n",
      " Look what we found in the park in the dark.\n",
      " We will take him home, we will call him Clark.\n",
      " He will live at our house, he will grow and grow.\n",
      " Will our mother like this? We don't know.\n",
      " And now, Good night.\n",
      " It is time to sleep\n",
      " So we will sleep with our pet Zeep.\n",
      " Today is gone. Today was fun.\n",
      " Tomorrow is another one.\n",
      " Every day, from here to there.\n",
      " funny things are everywhere.\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "9751e164-73d4-4c53-94d4-92f0d6f96b76",
   "metadata": {},
   "source": [
    "---\n",
    "## Pick Your Poison\n",
    "There are three texts provided for under `data/`. The first is:\n",
    "1. Dr. Seuss' \"One Fist, Two Fish\" (179 lines of text)\n",
    "2. All of Shakespeare's sonnets (2308 lines of text)\n",
    "3. Homer's \"The Odyssey\" (9255 Lines of text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1279dd44-0513-4cea-a51f-ac3b52927cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a more complex text that we can use to generate more complex output\n",
    "nth_order_markov_model = dict()\n",
    "with open(\"data/<YOUR_POISON>.txt\", \"r\") as poison_text:\n",
    "\n",
    "    # Process the lines. Consider that sonnets are separated by an empty line.\n",
    "    \n",
    "    nth_order_markov_model = build_markov_model(sonet_markov_model, corpus, order=2)\n",
    " \n",
    "print (generate_random_text(sonet_markov_model,seed=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db678920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
